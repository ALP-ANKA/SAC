{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sac_bipedalwalker",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/klinime/SAC/blob/master/sac_bipedalwalker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAHVB1BvSsXh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install box2d-py > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKY47oPSnrQ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_probability as tfp\n",
        "import time\n",
        "from datetime import datetime, timezone, timedelta\n",
        "import pickle\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "import logging\n",
        "logging.getLogger('pyvirtualdisplay').setLevel(level=logging.ERROR)\n",
        "from tensorflow.python.util import deprecation\n",
        "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "PATH = '/content/gdrive/My Drive/Colab Notebooks/sac_checkpoints/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHyChaXvC4zT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayPool:\n",
        "    def __init__(self, max_size, fields):\n",
        "        max_size = int(max_size)\n",
        "        self._max_size = max_size\n",
        "\n",
        "        self.fields = {}\n",
        "        self.field_names = []\n",
        "        self.add_fields(fields)\n",
        "\n",
        "        self._pointer = 0\n",
        "        self._size = 0\n",
        "\n",
        "    @property\n",
        "    def size(self):\n",
        "        return self._size\n",
        "\n",
        "    def add_fields(self, fields):\n",
        "        self.fields.update(fields)\n",
        "        self.field_names += list(fields.keys())\n",
        "\n",
        "        for field_name, field_attrs in fields.items():\n",
        "            field_shape = [self._max_size] + list(field_attrs['shape'])\n",
        "            initializer = field_attrs.get('initializer', np.zeros)\n",
        "            setattr(self, field_name, initializer(field_shape))\n",
        "\n",
        "    def _advance(self, count=1):\n",
        "        self._pointer = (self._pointer + count) % self._max_size\n",
        "        self._size = min(self._size + count, self._max_size)\n",
        "\n",
        "    def add_sample(self, **kwargs):\n",
        "        self.add_samples(1, **kwargs)\n",
        "\n",
        "    def add_samples(self, num_samples=1, **kwargs):\n",
        "        for field_name in self.field_names:\n",
        "            idx = np.arange(self._pointer,\n",
        "                            self._pointer + num_samples) % self._max_size\n",
        "            getattr(self, field_name)[idx] = kwargs.pop(field_name)\n",
        "\n",
        "        self._advance(num_samples)\n",
        "\n",
        "    def random_indices(self, batch_size):\n",
        "        if self._size == 0: return []\n",
        "        return np.random.randint(0, self._size, batch_size)\n",
        "\n",
        "    def random_batch(self, batch_size, field_name_filter=None):\n",
        "        random_indices = self.random_indices(batch_size)\n",
        "        return self.batch_by_indices(random_indices, field_name_filter)\n",
        "\n",
        "    def batch_by_indices(self, indices, field_name_filter=None):\n",
        "        field_names = self.field_names\n",
        "        if field_name_filter is not None:\n",
        "            field_names = [\n",
        "                field_name for field_name in field_names\n",
        "                if field_name_filter(field_name)\n",
        "            ]\n",
        "\n",
        "        return {\n",
        "            field_name: getattr(self, field_name)[indices]\n",
        "            for field_name in field_names\n",
        "        }\n",
        "    \n",
        "    def save(self):\n",
        "        with open(PATH + 'replay_pool.pkl', 'wb') as file:\n",
        "            pickle.dump(self, file, -1)\n",
        "            print('ReplayPool saved.')\n",
        "    \n",
        "    def load(self):\n",
        "        with open(PATH + 'replay_pool.pkl', 'rb') as file:\n",
        "            self = pickle.load(file)\n",
        "            print('ReplayPool loaded.')\n",
        "\n",
        "\n",
        "class SimpleReplayPool(ReplayPool):\n",
        "    def __init__(self, observation_shape, action_shape, *args, **kwargs):\n",
        "        self._observation_shape = observation_shape\n",
        "        self._action_shape = action_shape\n",
        "\n",
        "        fields = {\n",
        "            'observations': {\n",
        "                'shape': self._observation_shape,\n",
        "                'dtype': 'float32'\n",
        "            },\n",
        "            # It's a bit memory inefficient to save the observations twice,\n",
        "            # but it makes the code *much* easier since you no longer have\n",
        "            # to worry about termination conditions.\n",
        "            'next_observations': {\n",
        "                'shape': self._observation_shape,\n",
        "                'dtype': 'float32'\n",
        "            },\n",
        "            'actions': {\n",
        "                'shape': self._action_shape,\n",
        "                'dtype': 'float32'\n",
        "            },\n",
        "            'rewards': {\n",
        "                'shape': [],\n",
        "                'dtype': 'float32'\n",
        "            },\n",
        "            # self.terminals[i] = a terminal was received at time i\n",
        "            'terminals': {\n",
        "                'shape': [],\n",
        "                'dtype': 'bool'\n",
        "            },\n",
        "        }\n",
        "\n",
        "        super(SimpleReplayPool, self).__init__(*args, fields=fields, **kwargs)\n",
        "\n",
        "\n",
        "class Sampler():\n",
        "    def __init__(self, max_episode_length, prefill_steps=10000):\n",
        "        self._max_episode_length = max_episode_length\n",
        "        self._prefill_steps = prefill_steps\n",
        "\n",
        "        self.env = None\n",
        "        self.policy = None\n",
        "        self.pool = None\n",
        "\n",
        "    def initialize(self, env, policy, pool):\n",
        "        self.env = env\n",
        "        self.policy = policy\n",
        "        self.pool = pool\n",
        "\n",
        "        class UniformPolicy:\n",
        "            def __init__(self, action_dim):\n",
        "                self._action_dim = action_dim\n",
        "\n",
        "            def eval(self, _):\n",
        "                return np.random.uniform(-1, 1, self._action_dim)\n",
        "\n",
        "        uniform_exploration_policy = UniformPolicy(env.action_space.shape[0])\n",
        "        for _ in range(self._prefill_steps):\n",
        "            self.sample(uniform_exploration_policy)\n",
        "\n",
        "    def set_policy(self, policy):\n",
        "        self.policy = policy\n",
        "\n",
        "    def sample(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def random_batch(self, batch_size):\n",
        "        return self.pool.random_batch(batch_size)\n",
        "\n",
        "    def terminate(self):\n",
        "        self.env.terminate()\n",
        "\n",
        "\n",
        "class SimpleSampler(Sampler):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(SimpleSampler, self).__init__(**kwargs)\n",
        "\n",
        "        self._episode_length = 0\n",
        "        self._episode_return = 0\n",
        "        self._last_episode_return = 0\n",
        "        self._max_episode_return = -np.inf\n",
        "        self._n_episodes = 0\n",
        "        self._current_observation = None\n",
        "        self._total_samples = 0\n",
        "\n",
        "    def sample(self, policy=None):\n",
        "        policy = self.policy if policy is None else policy\n",
        "        if self._current_observation is None:\n",
        "            self._current_observation = self.env.reset()\n",
        "\n",
        "        action = policy.eval(self._current_observation)\n",
        "        next_observation, reward, terminal, info = self.env.step(action)\n",
        "        self._episode_length += 1\n",
        "        self._episode_return += reward\n",
        "        self._total_samples += 1\n",
        "\n",
        "        self.pool.add_sample(\n",
        "            observations=self._current_observation,\n",
        "            actions=action,\n",
        "            rewards=reward,\n",
        "            terminals=terminal,\n",
        "            next_observations=next_observation)\n",
        "\n",
        "        if terminal or self._episode_length >= self._max_episode_length:\n",
        "            self._current_observation = self.env.reset()\n",
        "            self._episode_length = 0\n",
        "            self._max_episode_return = max(self._max_episode_return,\n",
        "                                           self._episode_return)\n",
        "            self._last_episode_return = self._episode_return\n",
        "\n",
        "            self._episode_return = 0\n",
        "            self._n_episodes += 1\n",
        "\n",
        "        else:\n",
        "            self._current_observation = next_observation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iLvSE-JfQnk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_mlp_model(name, input_size, output_size, n_layers, size, activation='relu', output_activation=None):\n",
        "  model = keras.Sequential(name=name)\n",
        "  model.add(keras.layers.Dense(size, input_shape=(input_size,), activation=activation))\n",
        "  for _ in range(n_layers-1):\n",
        "    model.add(keras.layers.Dense(size, activation=activation))\n",
        "  model.add(keras.layers.Dense(output_size, activation=output_activation))\n",
        "  return model\n",
        "\n",
        "class GaussianPolicy(keras.Model):\n",
        "  def __init__(self, name, input_size, output_size, n_layers, size, activation='relu', output_activation=None):\n",
        "    super(GaussianPolicy, self).__init__(name=name)\n",
        "    self._f = None\n",
        "    self.mlp = build_mlp_model(name + '_mlp', input_size, output_size, n_layers, size, activation, output_activation)\n",
        "      \n",
        "  def call(self, obs_no):\n",
        "    mean, log_std = tf.split(self.mlp(obs_no), num_or_size_splits=2, axis=1)\n",
        "    log_std = tf.clip_by_value(log_std, -20., 2.)\n",
        "    distribution = tfp.distributions.MultivariateNormalDiag(loc=mean, scale_diag=tf.exp(log_std))\n",
        "    acs_na = distribution.sample()\n",
        "    logp_n = distribution.log_prob(acs_na)\n",
        "    logp_n -= self.squash_correction(acs_na)\n",
        "    return tf.tanh(acs_na), logp_n\n",
        "  \n",
        "  def squash_correction(self, acs_na):\n",
        "    return 2 * tf.reduce_sum(np.log(2) + acs_na - tf.nn.softplus(2 * acs_na), axis=1)\n",
        "      \n",
        "  def eval(self, observation):\n",
        "    assert self.built and observation.ndim == 1\n",
        "    if self._f is None:\n",
        "      self._f = keras.backend.function(self.inputs, [self.outputs[0]])\n",
        "    action, = self._f([observation[None]])\n",
        "    return action.flatten()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Cu_Xf-LoYGO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SAC_Trainer():\n",
        "  def __init__(self, sess, params):\n",
        "    self.sess = sess\n",
        "    self.env = params['env']\n",
        "    self.ob_dim = self.env.observation_space.shape[0]\n",
        "    self.ac_dim = self.env.action_space.shape[0]\n",
        "\n",
        "    self.alpha = params['alpha']\n",
        "    self.gamma = params['gamma']\n",
        "    self.tau = params['tau']\n",
        "    self.start_iter = params['start_iter']\n",
        "    self.n_iter = params['n_iter']\n",
        "    self.batch_size = params['batch_size']\n",
        "    self.ep_len = params['ep_len']\n",
        "\n",
        "    self.build()\n",
        "    self.log = None\n",
        "    self.log_freq = params['log_freq']\n",
        "    self.saver = tf.train.Saver(max_to_keep=self.n_iter//self.log_freq+1)\n",
        "  \n",
        "  def build(self):\n",
        "    models = self.define_models()\n",
        "    self.define_placeholders()\n",
        "    losses = self.define_losses()\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=params['learning_rate'], name='optimizer')\n",
        "    self.training_ops = [optimizer.minimize(loss=loss, var_list=model.trainable_variables) for loss, model in zip(losses, models)]\n",
        "    self.target_update_ops = self.define_target_update()\n",
        "  \n",
        "  def define_models(self):\n",
        "    self.policy = GaussianPolicy('policy', self.ob_dim, self.ac_dim * 2, params['n_layers'], params['size'])\n",
        "    self.v_func = build_mlp_model('v_func', self.ob_dim, 1, params['n_layers'], params['size'])\n",
        "    self.q_func1 = build_mlp_model('q_func1', self.ob_dim + self.ac_dim, 1, params['n_layers'], params['size'])\n",
        "    self.q_func2 = build_mlp_model('q_func2', self.ob_dim + self.ac_dim, 1, params['n_layers'], params['size'])\n",
        "    self.target_v_func = build_mlp_model('target_v_func', self.ob_dim, 1, params['n_layers'], params['size'])\n",
        "    return self.policy, self.v_func, self.q_func1, self.q_func2, self.target_v_func\n",
        "  \n",
        "  def define_placeholders(self):\n",
        "    self.obs_no_ph = tf.placeholder(tf.float32, shape=(None, self.ob_dim), name='obs_no')\n",
        "    self.acs_na_ph = tf.placeholder(tf.float32, shape=(None, self.ac_dim), name='acs_na')\n",
        "    self.rews_n_ph = tf.placeholder(tf.float32, shape=(None, ), name='rews_n')\n",
        "    self.next_obs_no_ph = tf.placeholder(tf.float32, shape=(None, self.ob_dim), name='next_obs_no')\n",
        "    self.terminals_n_ph = tf.placeholder(tf.float32, shape=(None, ), name='terminals_n')\n",
        "  \n",
        "  def define_losses(self):\n",
        "    acs_na, logp_n = self.policy(self.obs_no_ph)\n",
        "    q_input = tf.concat([self.obs_no_ph, self.acs_na_ph], axis=1)\n",
        "    q1_n = tf.squeeze(self.q_func1(q_input), axis=1)\n",
        "    q2_n = tf.squeeze(self.q_func2(q_input), axis=1)\n",
        "    q_n = tf.minimum(q1_n, q2_n)\n",
        "    v_n = tf.squeeze(self.v_func(self.obs_no_ph), axis=1)\n",
        "    target_v_n = tf.stop_gradient(q_n - self.alpha * logp_n)\n",
        "    target_q_n = tf.stop_gradient(self.rews_n_ph + \\\n",
        "        self.gamma * tf.squeeze(self.target_v_func(self.next_obs_no_ph), axis=1) * (1 - self.terminals_n_ph))\n",
        "\n",
        "    self.policy_loss = tf.reduce_mean(self.alpha * logp_n - q_n)\n",
        "    self.v_func_loss = tf.losses.mean_squared_error(target_v_n, v_n)\n",
        "    self.q_func1_loss = tf.losses.mean_squared_error(target_q_n, q1_n)\n",
        "    self.q_func2_loss = tf.losses.mean_squared_error(target_q_n, q2_n)\n",
        "    return self.policy_loss, self.v_func_loss, self.q_func1_loss, self.q_func2_loss\n",
        "  \n",
        "  def define_target_update(self):\n",
        "    return [tf.assign(target, (1 - self.tau) * target + self.tau * source)\n",
        "            for target, source in zip(self.target_v_func.trainable_variables, self.v_func.trainable_variables)]\n",
        "  \n",
        "  def eval_reward(self, sampler):\n",
        "    return np.sum(sampler.random_batch(self.batch_size)['rewards'])\n",
        "  \n",
        "  def save(self, sampler, i=None):\n",
        "    print('\\nSaving sesson...')\n",
        "    filename = 'iter_{:04d}_'.format(i) if i is not None else ''\n",
        "    pst_tz = timezone(timedelta(hours=-8), name='PST')\n",
        "    filename = filename + datetime.now(tz=pst_tz).strftime('%Y_%m_%d_%H_%M_%S') + '.ckpt'\n",
        "    self.saver.save(self.sess, PATH + filename)\n",
        "    np.save(PATH + 'log.npy', self.log)\n",
        "    print('Session saved.')\n",
        "    sampler.pool.save()\n",
        "    self.log_video(i)\n",
        "  \n",
        "  def load(self, filename):\n",
        "    self.saver.restore(self.sess, PATH + filename)\n",
        "    self.log = np.load(PATH + 'log.npy')\n",
        "    print('Session restored.')\n",
        "  \n",
        "  def log_video(self, i):\n",
        "    print('\\nLogging video...')\n",
        "    display = Display(visible=0, size=(1400, 900))\n",
        "    display.start()\n",
        "    rewards = []\n",
        "    env = gym.wrappers.Monitor(self.env, PATH + 'video/{:04d}'.format(i), video_callable=lambda episode_id: True, force=True)\n",
        "    ob = env.reset()\n",
        "    for t in range(self.ep_len):\n",
        "      env.render()\n",
        "      ac = self.policy.eval(ob)\n",
        "      ob, rew, done, _ = env.step(ac)\n",
        "      rewards.append(rew)\n",
        "      if done:\n",
        "        print('Episode finished in {} steps.'.format(t+1))\n",
        "        break\n",
        "    print('Total reward: {:.3f}'.format(np.sum(rewards)))\n",
        "    env.close()\n",
        "    print('Logging complete.')\n",
        "  \n",
        "  def run_training_loop(self, sampler):\n",
        "    start_time = time.time()\n",
        "    for itr in range(self.start_iter, self.start_iter + self.n_iter):\n",
        "      if itr % self.log_freq == 0:\n",
        "        print('\\n====================Iter {}/{}===================='.format(itr+1, self.n_iter))\n",
        "      for t in range(self.ep_len):\n",
        "        sampler.sample()\n",
        "        batch = sampler.random_batch(self.batch_size)\n",
        "        feed_dict = {\n",
        "            self.obs_no_ph: batch['observations'],\n",
        "            self.acs_na_ph: batch['actions'],\n",
        "            self.rews_n_ph: batch['rewards'],\n",
        "            self.next_obs_no_ph: batch['next_observations'],\n",
        "            self.terminals_n_ph: batch['terminals'],\n",
        "        }\n",
        "        self.sess.run(self.training_ops, feed_dict=feed_dict)\n",
        "        self.sess.run(self.target_update_ops)\n",
        "      \n",
        "      if itr % self.log_freq == 0:\n",
        "        print('\\nBeginning evaluation...')\n",
        "        reward = self.eval_reward(sampler)\n",
        "        if self.log is None:\n",
        "          self.log = np.array([[reward, time.time() - start_time]])\n",
        "        else:\n",
        "          self.log = np.concatenate([self.log, [[reward, time.time() - start_time]]])\n",
        "        print('EvalReward: {:.3f}'.format(self.log[-1][0]))\n",
        "        print('TimeElapsed: {:.3f}s'.format(self.log[-1][1]))\n",
        "        self.save(sampler, itr)\n",
        "    self.save(sampler, self.start_iter + self.n_iter)\n",
        "    print('Loop completed. Total time: {}'.format(time.time() - start_time))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikkJx791oe9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  params = {'env_name': 'BipedalWalker-v2',\n",
        "            'seed': 0,\n",
        "            'start_iter': 0,\n",
        "            'n_iter': 1000,\n",
        "            'batch_size': 256,\n",
        "            'ep_len': 1600,\n",
        "            'alpha': 0.2,\n",
        "            'gamma': 0.99,\n",
        "            'tau': 0.01,\n",
        "            'learning_rate': 5e-4,\n",
        "            'n_layers': 2,\n",
        "            'size': 256,\n",
        "            'log_freq': 50,\n",
        "            'load_checkpoint': None}\n",
        "  \n",
        "  seed = params['seed']\n",
        "  tf.set_random_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  env = gym.make(params['env_name'])\n",
        "  env.seed(seed)\n",
        "  params['env'] = env\n",
        "\n",
        "  trainer = SAC_Trainer(sess, params)\n",
        "  replay_pool = SimpleReplayPool(\n",
        "      observation_shape=env.observation_space.shape,\n",
        "      action_shape=env.action_space.shape,\n",
        "      max_size=1000000)\n",
        "  if params['load_checkpoint']:\n",
        "    trainer.load(params['load_checkpoint'])\n",
        "    replay_pool.load()\n",
        "    sampler = SimpleSampler(max_episode_length=params['ep_len'], prefill_steps=0)\n",
        "  else:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sampler = SimpleSampler(max_episode_length=params['ep_len'])\n",
        "  sampler.initialize(env, trainer.policy, replay_pool)\n",
        "  trainer.run_training_loop(sampler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkNoWN03LIar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "def render_video(filename):\n",
        "  video = io.open(PATH + filename, 'r+b').read()\n",
        "  encoded = base64.b64encode(video)\n",
        "  ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "              loop controls style=\"height: 400px;\">\n",
        "              <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "            </video>'''.format(encoded.decode('ascii'))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4MRgarqxMY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# render_video('video/0/openaigym.video.0.5301.video000000.mp4')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B50atfrRKoW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}